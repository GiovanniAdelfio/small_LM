{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "jmo-mkr2bGW7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmo-mkr2bGW7",
        "outputId": "e0e38f24-cb90-45c2-a943-2f341df9628e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'small_LM'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 152 (delta 67), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (152/152), 4.99 MiB | 4.63 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "/content/small_LM\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GiovanniAdelfio/small_LM\n",
        "%cd small_LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2224d1bb",
      "metadata": {
        "id": "2224d1bb"
      },
      "outputs": [],
      "source": [
        "## required imports\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "\n",
        "random.seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "path = os.getcwd() + os.sep + \"files\" + os.sep\n",
        "path_checkpoints = os.getcwd() + os.sep + \"checkpoints\" + os.sep"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data fetching"
      ],
      "metadata": {
        "id": "yp0vr8DmisTd"
      },
      "id": "yp0vr8DmisTd"
    },
    {
      "cell_type": "markdown",
      "id": "c449caed",
      "metadata": {
        "id": "c449caed"
      },
      "source": [
        "Selecting only \"Ordinary life\" dialogues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fbe596f5",
      "metadata": {
        "id": "fbe596f5"
      },
      "outputs": [],
      "source": [
        "used_lines = []\n",
        "\n",
        "with open(path + \"dialogues_topic.txt\", encoding=\"utf-8\") as topic:\n",
        "  for i, line in enumerate(topic):\n",
        "    if int(line) == 1:\n",
        "      used_lines += [i]\n",
        "lines = []\n",
        "\n",
        "with open(path + \"dialogues_text.txt\", encoding=\"utf-8\") as txt:\n",
        "  for i, el in enumerate(txt):\n",
        "    if i not in used_lines:\n",
        "      continue\n",
        "    lines.append(el)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9192201",
      "metadata": {
        "id": "f9192201"
      },
      "source": [
        "Choosing \"@\" as a token for the end of a person's sentence in the dialogue, and cleaning the sentences.\n",
        "\n",
        "We then concatenate the entire dataset into a single string: txt_chr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0ed7710",
      "metadata": {
        "id": "d0ed7710"
      },
      "outputs": [],
      "source": [
        "for i, el in enumerate(lines):\n",
        "  lines[i] = el.replace(\"\\n\", \"\")\n",
        "  lines[i] = lines[i].replace(\"__eou__\", \"@\")\n",
        "\n",
        "txt_chr = \"\".join(lines[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "471e1e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "471e1e9f",
        "outputId": "029e7daa-2d5f-4c3a-edef-84b7edb5f0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averege number of turns per dialog: 8\n"
          ]
        }
      ],
      "source": [
        "j=0\n",
        "for i in range(len(lines)):\n",
        "  j+= lines[i].count(\"@\")\n",
        "print(f\"Averege number of turns per dialog: {j//len(lines)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf2eadf",
      "metadata": {
        "id": "ecf2eadf"
      },
      "source": [
        "Creating a first encoding and decoding for our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a2610fa4",
      "metadata": {
        "id": "a2610fa4"
      },
      "outputs": [],
      "source": [
        "chars = set(txt_chr)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9f319e",
      "metadata": {
        "id": "1b9f319e"
      },
      "source": [
        "Converting our txt_chr into integers, following the encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2da894c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2da894c0",
        "outputId": "902d1fb6-8a6b-4e4a-fea3-949be8407d04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[18, 54, 64, 68, 28]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "txt_toi = []\n",
        "for chr in txt_chr:\n",
        "  txt_toi.append(stoi[chr])\n",
        "\n",
        "txt_toi[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data manipulation"
      ],
      "metadata": {
        "id": "8JJk7I8li4J5"
      },
      "id": "8JJk7I8li4J5"
    },
    {
      "cell_type": "markdown",
      "id": "f7e33e2c",
      "metadata": {
        "id": "f7e33e2c"
      },
      "source": [
        "We now train our tokenizer, we want to have a total of 1000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46d55e2",
      "metadata": {
        "id": "c46d55e2"
      },
      "outputs": [],
      "source": [
        "from tokenizer.tokenizer import token_train, merge, encode, decode\n",
        "\n",
        "num_chars = len(chars)\n",
        "new_tokens = 1000 - num_chars\n",
        "\n",
        "tkn_dataset, merges, itos = token_train(txt_toi, itos, num_chars, new_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ea08b7",
      "metadata": {
        "id": "85ea08b7"
      },
      "source": [
        "We check the compression rate of our tokenizer on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5e2720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc5e2720",
        "outputId": "cca3a745-3325-45fa-b05b-dd8c8499d905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compression rate: 70.81%\n"
          ]
        }
      ],
      "source": [
        "comp_rate = abs(len(tkn_dataset) - len(txt_toi))/len(txt_toi)\n",
        "print(f\"Compression rate: {comp_rate*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_chars = len(chars)\n",
        "new_tokens = 1000 - num_chars"
      ],
      "metadata": {
        "id": "NZp51hdb80oK"
      },
      "id": "NZp51hdb80oK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f3102018",
      "metadata": {
        "id": "f3102018"
      },
      "source": [
        "We now encode the dataset we will use for training, validation and testing of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "67b805f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67b805f8",
        "outputId": "d0cdb49c-4212-468d-c001-a7076b45607f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding and decoding works correctly!\n",
            "Average length of dialogs after compression: 135.89 tokens\n"
          ]
        }
      ],
      "source": [
        "dataset = encode(copy.deepcopy(lines), merges, stoi, num_chars, new_tokens)\n",
        "\n",
        "if decode(dataset[0], itos) == lines[0]:\n",
        "  print(\"Encoding and decoding works correctly!\")\n",
        "else :\n",
        "  print(\"There is an error in encoding and decoding.\")\n",
        "\n",
        "print(f\"Average length of dialogs after compression: {sum([len(x) for x in dataset])/len(dataset):.2f} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac1083a",
      "metadata": {
        "id": "eac1083a"
      },
      "source": [
        "We check some of the last tokens to ensure their meaningfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc1f3b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bc1f3b3",
        "outputId": "819b84f4-5d9f-4d06-d593-a19166cd7442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['. If ', '. @ Oh ', 'each ', 'name ', 'tal ', '. B', 'b ', '. @ Why ', 'wrong ', 'best ', 'whi', 'why ', 'coff', 'keep ', 'deli', '. @ We ', 'om ', '. @ Ok ', '. They ', ', th', 'might ', 'about the ', 'left ', 'another ', 'tri', 'feel ', 'oul', \"'d \", 'OK ', 'have the ']\n"
          ]
        }
      ],
      "source": [
        "print([itos[i] for i in range(970,1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c119c853",
      "metadata": {
        "id": "c119c853"
      },
      "source": [
        "We now save our \"stoi\", \"itos\", \"merges\" variables, needed for the encoding and decoding, and also the encoded dataset, for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f12c430",
      "metadata": {
        "id": "3f12c430"
      },
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', 'wb') as f:  # Open in binary write mode\n",
        "    pickle.dump([stoi, itos, merges, dataset], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f65f56",
      "metadata": {
        "id": "85f65f56"
      },
      "source": [
        "Here we can retrieve the saved data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0812a17e",
      "metadata": {
        "id": "0812a17e"
      },
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
        "    stoi, itos, merges, dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating datasets and dataloaders"
      ],
      "metadata": {
        "id": "2Dm9aFL-jDsy"
      },
      "id": "2Dm9aFL-jDsy"
    },
    {
      "cell_type": "markdown",
      "id": "70d0ee81",
      "metadata": {
        "id": "70d0ee81"
      },
      "source": [
        "We now create the target dataset from our inputs, by associating for each sequence of context_size lenght, the corresponding sequence in the text translated by one token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbacb1be",
      "metadata": {
        "id": "bbacb1be"
      },
      "source": [
        "We now divide the dataset in train, verification and test. We also trasform our datasets and targets into torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5ea4db92",
      "metadata": {
        "id": "5ea4db92"
      },
      "outputs": [],
      "source": [
        "from utils.data import split\n",
        "train_dataset, val_dataset, test_dataset = split(dataset, t=0.7, v=0.2, seed=42, to_torch = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedfc0af",
      "metadata": {
        "id": "eedfc0af"
      },
      "source": [
        "For consistency we save the randomly generated splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "921c30f1",
      "metadata": {
        "id": "921c30f1"
      },
      "outputs": [],
      "source": [
        "os.chdir(path)\n",
        "torch.save([train_dataset, val_dataset,test_dataset], \"dataset.pt\")\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4caf08fe",
      "metadata": {
        "id": "4caf08fe"
      },
      "outputs": [],
      "source": [
        "os.chdir(path)\n",
        "train_dataset, val_dataset, test_dataset = torch.load(\"dataset.pt\", weights_only= \"True\")\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h5mAWuvgWhfR",
      "metadata": {
        "id": "h5mAWuvgWhfR"
      },
      "source": [
        "We create a dataset and dataloader using pytorch utils, and wrap it on our files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "UJqXy_xQWg0U",
      "metadata": {
        "id": "UJqXy_xQWg0U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from utils.data import SLM_dataset\n",
        "bs = 256\n",
        "cs = 64\n",
        "\n",
        "train = SLM_dataset(train_dataset, cs)\n",
        "val = SLM_dataset(val_dataset, cs)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=bs, shuffle=False, num_workers=0)\n",
        "val_dataloader = DataLoader(val, batch_size=bs, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "LDrUmU19jRPl"
      },
      "id": "LDrUmU19jRPl"
    },
    {
      "cell_type": "markdown",
      "id": "6c6f3ae9",
      "metadata": {
        "id": "6c6f3ae9"
      },
      "source": [
        "We import our model, and generation function. We then initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "08c1b368",
      "metadata": {
        "id": "08c1b368"
      },
      "outputs": [],
      "source": [
        "from model.model import GPTModel, generate\n",
        "\n",
        "context_size = 64\n",
        "\n",
        "model = GPTModel(block_size=context_size, vocab_size=len(itos), n_embd=512, n_head=8, n_layer=6) #len itos, really? not good"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now perform 100 epochs of training."
      ],
      "metadata": {
        "id": "eguBHnELjphl"
      },
      "id": "eguBHnELjphl"
    },
    {
      "cell_type": "code",
      "source": [
        "from model.train import train\n",
        "\n",
        "model, train_loss, val_loss = train(model, train_dataloader, val_dataloader,\n",
        "                                    lr = 1e-3, weight_decay=1e-4, epochs=2,\n",
        "                                    opt_name=\"adam\", device=device, checkpoint_path=path_checkpoints)"
      ],
      "metadata": {
        "id": "vt3os2o3jpRz"
      },
      "id": "vt3os2o3jpRz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, \"Hey, how are you? @ \", 50, stoi, itos, merges, context_size, temperature= 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "K01emckgztFa",
        "outputId": "26567000-3c7c-44d8-d2e7-56119edfa10b"
      },
      "id": "K01emckgztFa",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey, how are you? @ Definitely not to respond my rules and you broke them correctly or scratch as the toilet is over there . @ It â€™ s just three hundred and seventy five '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}