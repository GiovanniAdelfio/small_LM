{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jmo-mkr2bGW7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmo-mkr2bGW7",
        "outputId": "f58dd7ee-50e7-4637-e673-c57471dfc1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'small_LM'...\n",
            "remote: Enumerating objects: 290, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/127)\u001b[K\rremote: Counting objects:   1% (2/127)\u001b[K\rremote: Counting objects:   2% (3/127)\u001b[K\rremote: Counting objects:   3% (4/127)\u001b[K\rremote: Counting objects:   4% (6/127)\u001b[K\rremote: Counting objects:   5% (7/127)\u001b[K\rremote: Counting objects:   6% (8/127)\u001b[K\rremote: Counting objects:   7% (9/127)\u001b[K\rremote: Counting objects:   8% (11/127)\u001b[K\rremote: Counting objects:   9% (12/127)\u001b[K\rremote: Counting objects:  10% (13/127)\u001b[K\rremote: Counting objects:  11% (14/127)\u001b[K\rremote: Counting objects:  12% (16/127)\u001b[K\rremote: Counting objects:  13% (17/127)\u001b[K\rremote: Counting objects:  14% (18/127)\u001b[K\rremote: Counting objects:  15% (20/127)\u001b[K\rremote: Counting objects:  16% (21/127)\u001b[K\rremote: Counting objects:  17% (22/127)\u001b[K\rremote: Counting objects:  18% (23/127)\u001b[K\rremote: Counting objects:  19% (25/127)\u001b[K\rremote: Counting objects:  20% (26/127)\u001b[K\rremote: Counting objects:  21% (27/127)\u001b[K\rremote: Counting objects:  22% (28/127)\u001b[K\rremote: Counting objects:  23% (30/127)\u001b[K\rremote: Counting objects:  24% (31/127)\u001b[K\rremote: Counting objects:  25% (32/127)\u001b[K\rremote: Counting objects:  26% (34/127)\u001b[K\rremote: Counting objects:  27% (35/127)\u001b[K\rremote: Counting objects:  28% (36/127)\u001b[K\rremote: Counting objects:  29% (37/127)\u001b[K\rremote: Counting objects:  30% (39/127)\u001b[K\rremote: Counting objects:  31% (40/127)\u001b[K\rremote: Counting objects:  32% (41/127)\u001b[K\rremote: Counting objects:  33% (42/127)\u001b[K\rremote: Counting objects:  34% (44/127)\u001b[K\rremote: Counting objects:  35% (45/127)\u001b[K\rremote: Counting objects:  36% (46/127)\u001b[K\rremote: Counting objects:  37% (47/127)\u001b[K\rremote: Counting objects:  38% (49/127)\u001b[K\rremote: Counting objects:  39% (50/127)\u001b[K\rremote: Counting objects:  40% (51/127)\u001b[K\rremote: Counting objects:  41% (53/127)\u001b[K\rremote: Counting objects:  42% (54/127)\u001b[K\rremote: Counting objects:  43% (55/127)\u001b[K\rremote: Counting objects:  44% (56/127)\u001b[K\rremote: Counting objects:  45% (58/127)\u001b[K\rremote: Counting objects:  46% (59/127)\u001b[K\rremote: Counting objects:  47% (60/127)\u001b[K\rremote: Counting objects:  48% (61/127)\u001b[K\rremote: Counting objects:  49% (63/127)\u001b[K\rremote: Counting objects:  50% (64/127)\u001b[K\rremote: Counting objects:  51% (65/127)\u001b[K\rremote: Counting objects:  52% (67/127)\u001b[K\rremote: Counting objects:  53% (68/127)\u001b[K\rremote: Counting objects:  54% (69/127)\u001b[K\rremote: Counting objects:  55% (70/127)\u001b[K\rremote: Counting objects:  56% (72/127)\u001b[K\rremote: Counting objects:  57% (73/127)\u001b[K\rremote: Counting objects:  58% (74/127)\u001b[K\rremote: Counting objects:  59% (75/127)\u001b[K\rremote: Counting objects:  60% (77/127)\u001b[K\rremote: Counting objects:  61% (78/127)\u001b[K\rremote: Counting objects:  62% (79/127)\u001b[K\rremote: Counting objects:  63% (81/127)\u001b[K\rremote: Counting objects:  64% (82/127)\u001b[K\rremote: Counting objects:  65% (83/127)\u001b[K\rremote: Counting objects:  66% (84/127)\u001b[K\rremote: Counting objects:  67% (86/127)\u001b[K\rremote: Counting objects:  68% (87/127)\u001b[K\rremote: Counting objects:  69% (88/127)\u001b[K\rremote: Counting objects:  70% (89/127)\u001b[K\rremote: Counting objects:  71% (91/127)\u001b[K\rremote: Counting objects:  72% (92/127)\u001b[K\rremote: Counting objects:  73% (93/127)\u001b[K\rremote: Counting objects:  74% (94/127)\u001b[K\rremote: Counting objects:  75% (96/127)\u001b[K\rremote: Counting objects:  76% (97/127)\u001b[K\rremote: Counting objects:  77% (98/127)\u001b[K\rremote: Counting objects:  78% (100/127)\u001b[K\rremote: Counting objects:  79% (101/127)\u001b[K\rremote: Counting objects:  80% (102/127)\u001b[K\rremote: Counting objects:  81% (103/127)\u001b[K\rremote: Counting objects:  82% (105/127)\u001b[K\rremote: Counting objects:  83% (106/127)\u001b[K\rremote: Counting objects:  84% (107/127)\u001b[K\rremote: Counting objects:  85% (108/127)\u001b[K\rremote: Counting objects:  86% (110/127)\u001b[K\rremote: Counting objects:  87% (111/127)\u001b[K\rremote: Counting objects:  88% (112/127)\u001b[K\rremote: Counting objects:  89% (114/127)\u001b[K\rremote: Counting objects:  90% (115/127)\u001b[K\rremote: Counting objects:  91% (116/127)\u001b[K\rremote: Counting objects:  92% (117/127)\u001b[K\rremote: Counting objects:  93% (119/127)\u001b[K\rremote: Counting objects:  94% (120/127)\u001b[K\rremote: Counting objects:  95% (121/127)\u001b[K\rremote: Counting objects:  96% (122/127)\u001b[K\rremote: Counting objects:  97% (124/127)\u001b[K\rremote: Counting objects:  98% (125/127)\u001b[K\rremote: Counting objects:  99% (126/127)\u001b[K\rremote: Counting objects: 100% (127/127)\u001b[K\rremote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 290 (delta 66), reused 11 (delta 11), pack-reused 163 (from 1)\u001b[K\n",
            "Receiving objects: 100% (290/290), 5.39 MiB | 20.66 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "/content/small_LM/small_LM\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GiovanniAdelfio/small_LM\n",
        "%cd small_LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2224d1bb",
      "metadata": {
        "id": "2224d1bb"
      },
      "outputs": [],
      "source": [
        "## required imports\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "\n",
        "random.seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "path = os.getcwd() + os.sep + \"files\" + os.sep\n",
        "path_checkpoints = os.getcwd() + os.sep + \"checkpoints\" + os.sep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9I9odNSRSfCL",
        "outputId": "0acceed3-00e5-468d-b5bd-45ca2b799a94"
      },
      "id": "9I9odNSRSfCL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/small_LM/small_LM/checkpoints/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yp0vr8DmisTd",
      "metadata": {
        "id": "yp0vr8DmisTd"
      },
      "source": [
        "## Data fetching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c449caed",
      "metadata": {
        "id": "c449caed"
      },
      "source": [
        "Selecting only \"Ordinary life\" dialogues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe596f5",
      "metadata": {
        "id": "fbe596f5"
      },
      "outputs": [],
      "source": [
        "used_lines = []\n",
        "\n",
        "with open(path + \"dialogues_topic.txt\", encoding=\"utf-8\") as topic:\n",
        "  for i, line in enumerate(topic):\n",
        "    if int(line) == 1:\n",
        "      used_lines += [i]\n",
        "lines = []\n",
        "\n",
        "with open(path + \"dialogues_text.txt\", encoding=\"utf-8\") as txt:\n",
        "  for i, el in enumerate(txt):\n",
        "    if i not in used_lines:\n",
        "      continue\n",
        "    lines.append(el)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9192201",
      "metadata": {
        "id": "f9192201"
      },
      "source": [
        "Choosing \"@\" as a token for the end of a person's sentence in the dialogue, and cleaning the sentences.\n",
        "\n",
        "We then concatenate the entire dataset into a single string: txt_chr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ed7710",
      "metadata": {
        "id": "d0ed7710"
      },
      "outputs": [],
      "source": [
        "for i, el in enumerate(lines):\n",
        "  lines[i] = el.replace(\"\\n\", \" \")\n",
        "  lines[i] = lines[i].replace(\"__eou__\", \"@\")\n",
        "\n",
        "txt_chr = \"\".join(lines[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471e1e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "471e1e9f",
        "outputId": "1c8802c7-71ca-4a62-90e6-b9beb60dd38b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averege number of turns per dialog: 8\n"
          ]
        }
      ],
      "source": [
        "j=0\n",
        "for i in range(len(lines)):\n",
        "  j+= lines[i].count(\"@\")\n",
        "print(f\"Averege number of turns per dialog: {j//len(lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_chr[:250]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "L5AkWxq5Ydgu",
        "outputId": "9c7d4b41-1921-4627-ba7d-71a7cdf4f8d2"
      },
      "id": "L5AkWxq5Ydgu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The kitchen stinks . @ I'll throw out the garbage . @ So Dick , how about getting some coffee for tonight ? @ Coffee ? I don ’ t honestly like that kind of stuff . @ Come on , you can at least try a little , besides your cigarette . @ What ’ s wrong \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf2eadf",
      "metadata": {
        "id": "ecf2eadf"
      },
      "source": [
        "Creating a first encoding and decoding for our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2610fa4",
      "metadata": {
        "id": "a2610fa4"
      },
      "outputs": [],
      "source": [
        "chars = set(txt_chr)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ign_chars_list = [ '!', '\"',\n",
        " '$',\n",
        " '%',\n",
        " '&',\n",
        " \"'\",\n",
        " '(',\n",
        " ')',\n",
        " ',',\n",
        " '-',\n",
        " '.',\n",
        " '/',\n",
        "'~',\n",
        " '£',\n",
        " '¥',\n",
        " '°',\n",
        " '–',\n",
        " '—',\n",
        " '‘',\n",
        " '’',\n",
        " '“',\n",
        " '”',\n",
        " '′',\n",
        " '。']"
      ],
      "metadata": {
        "id": "VqbB6VDRMnHb"
      },
      "id": "VqbB6VDRMnHb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1b9f319e",
      "metadata": {
        "id": "1b9f319e"
      },
      "source": [
        "Converting our txt_chr into integers, following the encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da894c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2da894c0",
        "outputId": "7c889110-4c46-4f16-adcf-197dee193941"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 63, 87, 74, 36]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "txt_toi = []\n",
        "ign_chars = set()\n",
        "for chr in txt_chr:\n",
        "  txt_toi.append(stoi[chr])\n",
        "for c in ign_chars_list:\n",
        "  ign_chars.add(stoi[c])\n",
        "txt_toi[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8JJk7I8li4J5",
      "metadata": {
        "id": "8JJk7I8li4J5"
      },
      "source": [
        "## Data manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e33e2c",
      "metadata": {
        "id": "f7e33e2c"
      },
      "source": [
        "We now train our tokenizer, we want to have a total of 1000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46d55e2",
      "metadata": {
        "id": "c46d55e2"
      },
      "outputs": [],
      "source": [
        "from tokenizer.tokenizer import token_train, merge, encode, decode\n",
        "\n",
        "num_chars = len(chars)\n",
        "new_tokens = 1000 - num_chars\n",
        "\n",
        "tkn_dataset, merges, itos = token_train(txt_toi, itos, num_chars, new_tokens, ign_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ea08b7",
      "metadata": {
        "id": "85ea08b7"
      },
      "source": [
        "We check the compression rate of our tokenizer on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5e2720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc5e2720",
        "outputId": "1af6bfb5-7288-4c7a-f2ed-2fcd614fe079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compression rate: 67.28%\n"
          ]
        }
      ],
      "source": [
        "comp_rate = abs(len(tkn_dataset) - len(txt_toi))/len(txt_toi)\n",
        "print(f\"Compression rate: {comp_rate*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3102018",
      "metadata": {
        "id": "f3102018"
      },
      "source": [
        "We now encode the dataset we will use for training, validation and testing of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b805f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67b805f8",
        "outputId": "c5d0210b-7bba-4095-f776-95902bb12f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding and decoding works correctly!\n",
            "Average length of dialogs after compression: 153.59 tokens\n"
          ]
        }
      ],
      "source": [
        "dataset = encode(copy.deepcopy(lines), merges, stoi, num_chars, new_tokens)\n",
        "\n",
        "if decode(dataset[0], itos) == lines[0]:\n",
        "  print(\"Encoding and decoding works correctly!\")\n",
        "else :\n",
        "  print(\"There is an error in encoding and decoding.\")\n",
        "\n",
        "print(f\"Average length of dialogs after compression: {sum([len(x) for x in dataset])/len(dataset):.2f} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac1083a",
      "metadata": {
        "id": "eac1083a"
      },
      "source": [
        "We check some of the last tokens to ensure their meaningfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc1f3b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bc1f3b3",
        "outputId": "833eed1c-5496-4877-f850-b15062743691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' F', 'coun', 'might ', 'about the ', 'left ', 'another ', 'feel ', 'oul', 'ck ', 'have the ', 'en I ', 'sma', 'tri', 'aybe ', 'differ', ' @ This ', 'cost ', 'ile ', ' @ Can you ', 'tast', 'sale ', ' Can you ', 'even ', 'ves ', 'tick', 'coffee ', 'ded ', 'rece', ' How much ', 'ty']\n"
          ]
        }
      ],
      "source": [
        "print([itos[i] for i in range(970,1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c119c853",
      "metadata": {
        "id": "c119c853"
      },
      "source": [
        "We now save our \"stoi\", \"itos\", \"merges\" variables, needed for the encoding and decoding, and also the encoded dataset, for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f12c430",
      "metadata": {
        "id": "3f12c430"
      },
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', 'wb') as f:  # Open in binary write mode\n",
        "    pickle.dump([stoi, itos, merges, dataset], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2Dm9aFL-jDsy",
      "metadata": {
        "id": "2Dm9aFL-jDsy"
      },
      "source": [
        "## Creating datasets and dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d0ee81",
      "metadata": {
        "id": "70d0ee81"
      },
      "source": [
        "We now create the target dataset from our inputs, by associating for each sequence of context_size lenght, the corresponding sequence in the text translated by one token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbacb1be",
      "metadata": {
        "id": "bbacb1be"
      },
      "source": [
        "We now divide the dataset in train, verification and test. We also trasform our datasets and targets into torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea4db92",
      "metadata": {
        "id": "5ea4db92"
      },
      "outputs": [],
      "source": [
        "from utils.data import split\n",
        "train_dataset, val_dataset, test_dataset = split(dataset, t=0.7, v=0.2, seed=42, to_torch = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedfc0af",
      "metadata": {
        "id": "eedfc0af"
      },
      "source": [
        "For consistency we save the randomly generated splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921c30f1",
      "metadata": {
        "id": "921c30f1"
      },
      "outputs": [],
      "source": [
        "os.chdir(path)\n",
        "torch.save([train_dataset, val_dataset,test_dataset], \"dataset.pt\")\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4caf08fe",
      "metadata": {
        "id": "4caf08fe"
      },
      "outputs": [],
      "source": [
        "os.chdir(path)\n",
        "train_dataset, val_dataset, test_dataset = torch.load(\"dataset.pt\", weights_only= \"True\")\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h5mAWuvgWhfR",
      "metadata": {
        "id": "h5mAWuvgWhfR"
      },
      "source": [
        "We create a dataset and dataloader using pytorch utils, and wrap it on our files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UJqXy_xQWg0U",
      "metadata": {
        "id": "UJqXy_xQWg0U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from utils.data import SLM_dataset\n",
        "bs = 32\n",
        "cs = 64\n",
        "\n",
        "train = SLM_dataset(train_dataset, cs)\n",
        "val = SLM_dataset(val_dataset, cs)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=bs, shuffle=True, num_workers=0)\n",
        "val_dataloader = DataLoader(val, batch_size=bs, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LDrUmU19jRPl",
      "metadata": {
        "id": "LDrUmU19jRPl"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c6f3ae9",
      "metadata": {
        "id": "6c6f3ae9"
      },
      "source": [
        "We import our model, and generation function. We then initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c1b368",
      "metadata": {
        "id": "08c1b368"
      },
      "outputs": [],
      "source": [
        "from model.model import GPTModel, generate\n",
        "\n",
        "context_size = 64\n",
        "\n",
        "model = GPTModel(block_size=context_size, vocab_size=1000, n_embd=512, n_head=8, n_layer=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac871ca3",
      "metadata": {
        "id": "ac871ca3"
      },
      "source": [
        "We can load saved weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c438a7",
      "metadata": {
        "id": "59c438a7",
        "outputId": "2415beb0-1b90-4a4b-f719-993a1f727411"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = \"test1_epoch6.pt\"  # specify your checkpoint file here\n",
        "model.load_state_dict(torch.load(path_checkpoints + checkpoint, map_location=device, weights_only= False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eguBHnELjphl",
      "metadata": {
        "id": "eguBHnELjphl"
      },
      "source": [
        "We now perform 100 epochs of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vt3os2o3jpRz",
      "metadata": {
        "id": "vt3os2o3jpRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbbd072-f3da-43fa-b096-ba3515123744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 231/231 [00:19<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 2.6492 | Val Loss = 3.0657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 231/231 [00:20<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 2.5806 | Val Loss = 3.0546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 231/231 [00:19<00:00, 11.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 2.5485 | Val Loss = 3.0481\n",
            "Training completato!\n"
          ]
        }
      ],
      "source": [
        "from model.train import train\n",
        "\n",
        "model, train_loss, val_loss = train(model, train_dataloader, val_dataloader,\n",
        "                                    lr = 1e-4, weight_decay=1e-3, epochs=3,\n",
        "                                    opt_name=\"adam\", device=device,\n",
        "                                    checkpoint_path=path_checkpoints, name = \"new_tkn_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bed8986",
      "metadata": {
        "id": "8bed8986"
      },
      "source": [
        "Here we load the saved dicts, needed for encoding and decoding during generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0812a17e",
      "metadata": {
        "id": "0812a17e"
      },
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
        "    stoi, itos, merges, dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K01emckgztFa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "K01emckgztFa",
        "outputId": "d77d151a-d511-42d3-c62e-bf1b66cd6532"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm going to buy a new pair of shoes . @ surf the second . @ Because the soft boiling is better . @ OK . @ Very good . @ Bye . @ Yeah , right ? I'm glad you know . S\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "generate(model, \" I'm going to buy a new pair of shoes . @ \",\n",
        "         50, stoi, itos, merges, context_size, conversation=False, temperature= 0.6, top_k=100, top_p = 0.8)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}