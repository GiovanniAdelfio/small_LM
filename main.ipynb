{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2224d1bb",
      "metadata": {
        "id": "2224d1bb"
      },
      "outputs": [],
      "source": [
        "## required imports\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c449caed",
      "metadata": {
        "id": "c449caed"
      },
      "source": [
        "Selecting only \"Ordinary life\" dialogues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fbe596f5",
      "metadata": {
        "id": "fbe596f5"
      },
      "outputs": [],
      "source": [
        "used_lines = []\n",
        "\n",
        "with open(\"dialogues_topic.txt\") as topic:\n",
        "  for i, line in enumerate(topic):\n",
        "    if int(line) == 1:\n",
        "      used_lines += [i]\n",
        "lines = []\n",
        "\n",
        "with open(\"dialogues_text.txt\") as txt:\n",
        "  for i, el in enumerate(txt):\n",
        "    if i not in used_lines:\n",
        "      continue\n",
        "    lines.append(el)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9192201",
      "metadata": {
        "id": "f9192201"
      },
      "source": [
        "Choosing \"@\" as a token for the end of a person's sentence in the dialogue, and cleaning the sentences.\n",
        "\n",
        "We then concatenate the entire dataset into a single string: txt_chr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0ed7710",
      "metadata": {
        "id": "d0ed7710"
      },
      "outputs": [],
      "source": [
        "for i, el in enumerate(lines):\n",
        "  lines[i] = el.replace(\"\\n\", \"\")\n",
        "  lines[i] = lines[i].replace(\"__eou__\", \"@\")\n",
        "\n",
        "txt_chr = \"\".join(lines[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "471e1e9f",
      "metadata": {
        "id": "471e1e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c9b9d1-334f-48d2-8373-6315c6b3626b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averege number of turns per dialog: 8\n"
          ]
        }
      ],
      "source": [
        "j=0\n",
        "for i in range(len(lines)):\n",
        "  j+= lines[i].count(\"@\")\n",
        "print(f\"Averege number of turns per dialog: {j//len(lines)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf2eadf",
      "metadata": {
        "id": "ecf2eadf"
      },
      "source": [
        "Creating a first encoding and decoding for our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a2610fa4",
      "metadata": {
        "id": "a2610fa4"
      },
      "outputs": [],
      "source": [
        "chars = set(txt_chr)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9f319e",
      "metadata": {
        "id": "1b9f319e"
      },
      "source": [
        "Converting our txt_chr into integers, following the encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2da894c0",
      "metadata": {
        "id": "2da894c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f75a3f-80c9-49e3-d31c-58cf577b2ad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[62, 2, 54, 25, 32]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "txt_toi = []\n",
        "for chr in txt_chr:\n",
        "  txt_toi.append(stoi[chr])\n",
        "\n",
        "txt_toi[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e33e2c",
      "metadata": {
        "id": "f7e33e2c"
      },
      "source": [
        "We now train our tokenizer, we want to have a total of 1000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46d55e2",
      "metadata": {
        "id": "c46d55e2"
      },
      "outputs": [],
      "source": [
        "from tokenizer.tokenizer import token_train, merge, encode, decode\n",
        "\n",
        "num_chars = len(chars)\n",
        "new_tokens = 1000 - num_chars\n",
        "\n",
        "tkn_dataset, merges, itos = token_train(txt_toi, itos, num_chars, new_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ea08b7",
      "metadata": {
        "id": "85ea08b7"
      },
      "source": [
        "We check the compression rate of our tokenizer on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5e2720",
      "metadata": {
        "id": "dc5e2720"
      },
      "outputs": [],
      "source": [
        "comp_rate = abs(len(tkn_dataset) - len(txt_toi))/len(txt_toi)\n",
        "print(f\"Compression rate: {comp_rate*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3102018",
      "metadata": {
        "id": "f3102018"
      },
      "source": [
        "We now encode the dataset we will use for training, validation and testing of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b805f8",
      "metadata": {
        "id": "67b805f8"
      },
      "outputs": [],
      "source": [
        "dataset = encode(copy.deepcopy(lines), merges, stoi, num_chars, new_tokens)\n",
        "\n",
        "if decode(dataset[0], merges, itos) == lines[0]:\n",
        "  print(\"Encoding and decoding works correctly!\")\n",
        "else :\n",
        "  print(\"There is an error in encoding and decoding.\")\n",
        "\n",
        "print(f\"Average length of dialogs after compression: {sum([len(x) for x in dataset])/len(dataset):.2f} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac1083a",
      "metadata": {
        "id": "eac1083a"
      },
      "source": [
        "We check some of the last tokens to ensure their meaningfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc1f3b3",
      "metadata": {
        "id": "8bc1f3b3"
      },
      "outputs": [],
      "source": [
        "print([itos[i] for i in range(970,1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c119c853",
      "metadata": {
        "id": "c119c853"
      },
      "source": [
        "We now save our \"stoi\", \"itos\", \"merges\" variables, needed for the encoding and decoding, and also the encoded dataset, for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f12c430",
      "metadata": {
        "id": "3f12c430"
      },
      "outputs": [],
      "source": [
        "with open('stoi_itos_merges_dataset.pkl', 'wb') as f:  # Open in binary write mode\n",
        "    pickle.dump([stoi, itos, merges, dataset], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f65f56",
      "metadata": {
        "id": "85f65f56"
      },
      "source": [
        "Here we can retrieve the saved data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0812a17e",
      "metadata": {
        "id": "0812a17e"
      },
      "outputs": [],
      "source": [
        "with open('stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
        "    stoi, itos, merges, dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d0ee81",
      "metadata": {
        "id": "70d0ee81"
      },
      "source": [
        "We now create the target dataset from our inputs, by associating for each sequence of context_size lenght, the corresponding sequence in the text translated by one token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aeebbfd",
      "metadata": {
        "id": "0aeebbfd"
      },
      "outputs": [],
      "source": [
        "target = []\n",
        "context_size = 64\n",
        "\n",
        "for dialog in dataset:\n",
        "    for i in range(len(dialog) - context_size):\n",
        "        target_seq = dialog[i + 1:i + context_size + 1]\n",
        "        target.append(target_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbacb1be",
      "metadata": {
        "id": "bbacb1be"
      },
      "source": [
        "We now divide the dataset in train, verification and test. We also trasform our datasets and targets into torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea4db92",
      "metadata": {
        "id": "5ea4db92"
      },
      "outputs": [],
      "source": [
        "from utils.data import split\n",
        "train_dataset, val_dataset, test_dataset, train_target, val_target, test_target = split(dataset, target, t=0.7, v=0.2, seed=42, to_torch = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedfc0af",
      "metadata": {
        "id": "eedfc0af"
      },
      "source": [
        "For consistency we save the randomly generated splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921c30f1",
      "metadata": {
        "id": "921c30f1"
      },
      "outputs": [],
      "source": [
        "torch.save([train_dataset, val_dataset,test_dataset], \"dataset.pt\")\n",
        "torch.save([train_target, val_target, test_target], \"target.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4caf08fe",
      "metadata": {
        "id": "4caf08fe"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = torch.load(\"dataset.pt\")\n",
        "train_target, val_target, test_target = torch.load(\"target.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a dataset and dataloader using pytorch utils, and wrap it on our files."
      ],
      "metadata": {
        "id": "h5mAWuvgWhfR"
      },
      "id": "h5mAWuvgWhfR"
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.data import SLM_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train = SML_dataset(train_dataset, train_target)\n",
        "val = SML_dataset(val_dataset, val_target)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "UJqXy_xQWg0U"
      },
      "id": "UJqXy_xQWg0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6c6f3ae9",
      "metadata": {
        "id": "6c6f3ae9"
      },
      "source": [
        "We import our model, and generation function. We then initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c1b368",
      "metadata": {
        "id": "08c1b368"
      },
      "outputs": [],
      "source": [
        "from model.model import GPTModel, generate\n",
        "\n",
        "model = GPTModel(block_size=context_size, vocab_size=len(itos), n_embd=512, n_head=8, n_layer=6)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}