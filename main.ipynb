{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## required imports\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449caed",
   "metadata": {},
   "source": [
    "Selecting only \"Ordinary life\" dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe596f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_lines = []\n",
    "\n",
    "with open(\"dialogues_topic.txt\") as topic:\n",
    "  for i, line in enumerate(topic):\n",
    "    if int(line) == 1:\n",
    "      used_lines += [i]\n",
    "lines = []\n",
    "\n",
    "with open(\"dialogues_text.txt\") as txt:\n",
    "  for i, el in enumerate(txt):\n",
    "    if i not in used_lines:\n",
    "      continue\n",
    "    lines.append(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9192201",
   "metadata": {},
   "source": [
    "Choosing \"@\" as a token for the end of a person's sentence in the dialogue, and cleaning the sentences. \n",
    "\n",
    "We then concatenate the entire dataset into a single string: txt_chr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, el in enumerate(lines):\n",
    "  lines[i] = el.replace(\"\\n\", \"\")\n",
    "  lines[i] = lines[i].replace(\"__eou__\", \"@\")\n",
    "  \n",
    "txt_chr = \"\".join(lines[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in range(len(lines)):\n",
    "  j+= lines[i].count(\"@\")\n",
    "print(f\"Averege number of turns per dialog: {j//len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2eadf",
   "metadata": {},
   "source": [
    "Creating a first encoding and decoding for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2610fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(txt_chr)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f319e",
   "metadata": {},
   "source": [
    "Converting our txt_chr into integers, following the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da894c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_toi = []\n",
    "for chr in txt_chr:\n",
    "  txt_toi.append(stoi[chr])\n",
    "  \n",
    "txt_toi[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e33e2c",
   "metadata": {},
   "source": [
    "We now train our tokenizer, we want to have a total of 1000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer.tokenizer import token_train, merge, encode, decode\n",
    "\n",
    "num_chars = len(chars)\n",
    "new_tokens = 1000 - num_chars\n",
    "\n",
    "tkn_dataset, merges, itos = token_train(txt_toi, itos, num_chars, new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea08b7",
   "metadata": {},
   "source": [
    "We check the compression rate of our tokenizer on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_rate = abs(len(tkn_dataset) - len(txt_toi))/len(txt_toi)\n",
    "print(f\"Compression rate: {comp_rate*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3102018",
   "metadata": {},
   "source": [
    "We now encode the dataset we will use for training, validation and testing of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = encode(copy.deepcopy(lines), merges, stoi, num_chars, new_tokens)\n",
    "\n",
    "if decode(dataset[0], merges, itos) == lines[0]:\n",
    "  print(\"Encoding and decoding works correctly!\")\n",
    "else :\n",
    "  print(\"There is an error in encoding and decoding.\")\n",
    "\n",
    "print(f\"Average length of dialogs after compression: {sum([len(x) for x in dataset])/len(dataset):.2f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1083a",
   "metadata": {},
   "source": [
    "We check some of the last tokens to ensure their meaningfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([itos[i] for i in range(970,1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119c853",
   "metadata": {},
   "source": [
    "We now save our \"stoi\", \"itos\", \"merges\" variables, needed for the encoding and decoding, and also the encoded dataset, for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stoi_itos_merges_dataset.pkl', 'wb') as f:  # Open in binary write mode\n",
    "    pickle.dump([stoi, itos, merges, dataset], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f65f56",
   "metadata": {},
   "source": [
    "Here we can retrieve the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
    "    stoi, itos, merges, dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0ee81",
   "metadata": {},
   "source": [
    "We now create the target dataset from our inputs, by associating for each sequence of context_size lenght, the corresponding sequence in the text translated by one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeebbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "context_size = 64\n",
    "\n",
    "for dialog in dataset:\n",
    "    for i in range(len(dialog) - context_size):\n",
    "        target_seq = dialog[i + 1:i + context_size + 1]\n",
    "        target.append(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacb1be",
   "metadata": {},
   "source": [
    "We now divide the dataset in train, verification and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "n_train = int(n * 0.7)\n",
    "n_val = int(n * 0.2)\n",
    "\n",
    "indices = list(range(n))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_target = [target[i] for i in indices[:n_train]]\n",
    "val_target = [target[i] for i in indices[n_train:n_train + n_val]]\n",
    "test_target = [target[i] for i in indices[n_train + n_val:]]\n",
    "\n",
    "train_dataset = [dataset[i] for i in indices[:n_train]]\n",
    "val_dataset = [dataset[i] for i in indices[n_train:n_train + n_val]]\n",
    "test_dataset = [dataset[i] for i in indices[n_train + n_val:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfc0af",
   "metadata": {},
   "source": [
    "For consistency we save the randomly generated splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_val_test.pkl', 'wb') as f:  # Open in binary write mode\n",
    "    pickle.dump([train_dataset, val_dataset, test_dataset], f)\n",
    "with open('train_val_test_target.pkl', 'wb') as f:  # Open in binary write mode\n",
    "    pickle.dump([train_target, val_target, test_target], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
    "    train_dataset, val_dataset, test_dataset = pickle.load(f)\n",
    "with open('train_val_test_target.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
    "    train_target, val_target, test_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93493542",
   "metadata": {},
   "source": [
    "We now trasform our datasets and targets into torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc09c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [torch.tensor(seq, dtype=torch.long).to(device) for seq in train_dataset]\n",
    "val_dataset = [torch.tensor(seq, dtype=torch.long).to(device) for seq in val_dataset]\n",
    "test_dataset = [torch.tensor(seq, dtype=torch.long).to(device) for seq in test_dataset]    \n",
    "\n",
    "train_target = [torch.tensor(seq, dtype=torch.long).to(device) for seq in train_target]\n",
    "val_target = [torch.tensor(seq, dtype=torch.long).to(device) for seq in val_target]\n",
    "test_target = [torch.tensor(seq, dtype=torch.long).to(device) for seq in test_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f3ae9",
   "metadata": {},
   "source": [
    "We import our model, and generation function. We then initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import GPTModel, generate\n",
    "\n",
    "model = GPTModel(block_size=context_size, vocab_size=len(itos), n_embd=512, n_head=8, n_layer=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
