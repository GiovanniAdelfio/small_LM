{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "jmo-mkr2bGW7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmo-mkr2bGW7",
        "outputId": "f58dd7ee-50e7-4637-e673-c57471dfc1f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'small_LM'...\n",
            "remote: Enumerating objects: 316, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 316 (delta 76), reused 11 (delta 11), pack-reused 163 (from 1)\u001b[K\n",
            "Receiving objects: 100% (316/316), 7.01 MiB | 7.15 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "/home/simone/jupyter_files/small_LM\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GiovanniAdelfio/small_LM\n",
        "%cd small_LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2224d1bb",
      "metadata": {
        "id": "2224d1bb"
      },
      "outputs": [],
      "source": [
        "## required imports\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "\n",
        "random.seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "path = os.getcwd() + os.sep + \"files\" + os.sep\n",
        "path_checkpoints = os.getcwd() + os.sep + \"checkpoints\" + os.sep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yp0vr8DmisTd",
      "metadata": {
        "id": "yp0vr8DmisTd"
      },
      "source": [
        "## Data fetching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c449caed",
      "metadata": {
        "id": "c449caed"
      },
      "source": [
        "Selecting only \"Ordinary life\" dialogues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe596f5",
      "metadata": {
        "id": "fbe596f5"
      },
      "outputs": [],
      "source": [
        "used_lines = []\n",
        "\n",
        "with open(path + \"dialogues_topic.txt\", encoding=\"utf-8\") as topic:\n",
        "  for i, line in enumerate(topic):\n",
        "    if int(line) == 1:\n",
        "      used_lines += [i]\n",
        "lines = []\n",
        "\n",
        "with open(path + \"dialogues_text.txt\", encoding=\"utf-8\") as txt:\n",
        "  for i, el in enumerate(txt):\n",
        "    if i not in used_lines:\n",
        "      continue\n",
        "    lines.append(el)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9192201",
      "metadata": {
        "id": "f9192201"
      },
      "source": [
        "Choosing \"@\" as a token for the end of a person's sentence in the dialogue, and cleaning the sentences.\n",
        "\n",
        "We then concatenate the entire dataset into a single string: txt_chr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ed7710",
      "metadata": {
        "id": "d0ed7710"
      },
      "outputs": [],
      "source": [
        "for i, el in enumerate(lines):\n",
        "  lines[i] = el.replace(\"\\n\", \" \")\n",
        "  lines[i] = lines[i].replace(\"__eou__\", \"@\")\n",
        "\n",
        "txt_chr = \"\".join(lines[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471e1e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "471e1e9f",
        "outputId": "1c8802c7-71ca-4a62-90e6-b9beb60dd38b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averege number of turns per dialog: 8\n"
          ]
        }
      ],
      "source": [
        "j=0\n",
        "for i in range(len(lines)):\n",
        "  j+= lines[i].count(\"@\")\n",
        "print(f\"Averege number of turns per dialog: {j//len(lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L5AkWxq5Ydgu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "L5AkWxq5Ydgu",
        "outputId": "9c7d4b41-1921-4627-ba7d-71a7cdf4f8d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The kitchen stinks . @ I'll throw out the garbage . @ So Dick , how about getting some coffee for tonight ? @ Coffee ? I don ‚Äô t honestly like that kind of stuff . @ Come on , you can at least try a little , besides your cigarette . @ What ‚Äô s wrong \""
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt_chr[:250]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf2eadf",
      "metadata": {
        "id": "ecf2eadf"
      },
      "source": [
        "Creating a first encoding and decoding for our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2610fa4",
      "metadata": {
        "id": "a2610fa4"
      },
      "outputs": [],
      "source": [
        "chars = set(txt_chr)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VqbB6VDRMnHb",
      "metadata": {
        "id": "VqbB6VDRMnHb"
      },
      "outputs": [],
      "source": [
        "ign_chars_list = [ '!', '\"',\n",
        " '$',\n",
        " '%',\n",
        " '&',\n",
        " \"'\",\n",
        " '(',\n",
        " ')',\n",
        " ',',\n",
        " '-',\n",
        " '.',\n",
        " '/',\n",
        "'~',\n",
        " '¬£',\n",
        " '¬•',\n",
        " '¬∞',\n",
        " '‚Äì',\n",
        " '‚Äî',\n",
        " '‚Äò',\n",
        " '‚Äô',\n",
        " '‚Äú',\n",
        " '‚Äù',\n",
        " '‚Ä≤',\n",
        " '„ÄÇ']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9f319e",
      "metadata": {
        "id": "1b9f319e"
      },
      "source": [
        "Converting our txt_chr into integers, following the encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da894c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2da894c0",
        "outputId": "7c889110-4c46-4f16-adcf-197dee193941"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10, 63, 87, 74, 36]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt_toi = []\n",
        "ign_chars = set()\n",
        "for chr in txt_chr:\n",
        "  txt_toi.append(stoi[chr])\n",
        "for c in ign_chars_list:\n",
        "  ign_chars.add(stoi[c])\n",
        "txt_toi[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8JJk7I8li4J5",
      "metadata": {
        "id": "8JJk7I8li4J5"
      },
      "source": [
        "## Data manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e33e2c",
      "metadata": {
        "id": "f7e33e2c"
      },
      "source": [
        "We now train our tokenizer, we want to have a total of 1000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46d55e2",
      "metadata": {
        "id": "c46d55e2"
      },
      "outputs": [],
      "source": [
        "from tokenizer.tokenizer import token_train, merge, encode, decode\n",
        "\n",
        "num_chars = len(chars)\n",
        "new_tokens = 1000 - num_chars\n",
        "\n",
        "tkn_dataset, merges, itos = token_train(txt_toi, itos, num_chars, new_tokens, ign_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ea08b7",
      "metadata": {
        "id": "85ea08b7"
      },
      "source": [
        "We check the compression rate of our tokenizer on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5e2720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc5e2720",
        "outputId": "1af6bfb5-7288-4c7a-f2ed-2fcd614fe079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compression rate: 67.28%\n"
          ]
        }
      ],
      "source": [
        "comp_rate = abs(len(tkn_dataset) - len(txt_toi))/len(txt_toi)\n",
        "print(f\"Compression rate: {comp_rate*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3102018",
      "metadata": {
        "id": "f3102018"
      },
      "source": [
        "We now encode the dataset we will use for training, validation and testing of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b805f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67b805f8",
        "outputId": "c5d0210b-7bba-4095-f776-95902bb12f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding and decoding works correctly!\n",
            "Average length of dialogs after compression: 153.59 tokens\n"
          ]
        }
      ],
      "source": [
        "dataset = encode(copy.deepcopy(lines), merges, stoi, num_chars, new_tokens)\n",
        "\n",
        "if decode(dataset[0], itos) == lines[0]:\n",
        "  print(\"Encoding and decoding works correctly!\")\n",
        "else :\n",
        "  print(\"There is an error in encoding and decoding.\")\n",
        "\n",
        "print(f\"Average length of dialogs after compression: {sum([len(x) for x in dataset])/len(dataset):.2f} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac1083a",
      "metadata": {
        "id": "eac1083a"
      },
      "source": [
        "We check some of the last tokens to ensure their meaningfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc1f3b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bc1f3b3",
        "outputId": "833eed1c-5496-4877-f850-b15062743691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' F', 'coun', 'might ', 'about the ', 'left ', 'another ', 'feel ', 'oul', 'ck ', 'have the ', 'en I ', 'sma', 'tri', 'aybe ', 'differ', ' @ This ', 'cost ', 'ile ', ' @ Can you ', 'tast', 'sale ', ' Can you ', 'even ', 'ves ', 'tick', 'coffee ', 'ded ', 'rece', ' How much ', 'ty']\n"
          ]
        }
      ],
      "source": [
        "print([itos[i] for i in range(970,1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c119c853",
      "metadata": {
        "id": "c119c853"
      },
      "source": [
        "We now save our \"stoi\", \"itos\", \"merges\" variables, needed for the encoding and decoding, and also the encoded dataset, for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f12c430",
      "metadata": {
        "id": "3f12c430"
      },
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', 'wb') as f:  # Open in binary write mode\n",
        "    pickle.dump([stoi, itos, merges, dataset], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2Dm9aFL-jDsy",
      "metadata": {
        "id": "2Dm9aFL-jDsy"
      },
      "source": [
        "## Creating datasets and dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d0ee81",
      "metadata": {
        "id": "70d0ee81"
      },
      "source": [
        "We now create the target dataset from our inputs, by associating for each sequence of context_size lenght, the corresponding sequence in the text translated by one token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbacb1be",
      "metadata": {
        "id": "bbacb1be"
      },
      "source": [
        "We now divide the dataset in train, verification and test. We also trasform our datasets and targets into torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea4db92",
      "metadata": {
        "id": "5ea4db92"
      },
      "outputs": [],
      "source": [
        "from utils.data import split\n",
        "train_dataset, val_dataset, test_dataset = split(dataset, t=0.7, v=0.2, seed=42, to_torch = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedfc0af",
      "metadata": {
        "id": "eedfc0af"
      },
      "source": [
        "For consistency we save the randomly generated splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921c30f1",
      "metadata": {
        "id": "921c30f1"
      },
      "outputs": [],
      "source": [
        "os.chdir(path)\n",
        "torch.save([train_dataset, val_dataset,test_dataset], \"dataset.pt\")\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4caf08fe",
      "metadata": {
        "id": "4caf08fe"
      },
      "outputs": [],
      "source": [
        "os.chdir(path)\n",
        "train_dataset, val_dataset, test_dataset = torch.load(\"dataset.pt\", weights_only= \"True\")\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h5mAWuvgWhfR",
      "metadata": {
        "id": "h5mAWuvgWhfR"
      },
      "source": [
        "We create a dataset and dataloader using pytorch utils, and wrap it on our files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "UJqXy_xQWg0U",
      "metadata": {
        "id": "UJqXy_xQWg0U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from utils.data import SLM_dataset\n",
        "bs = 32\n",
        "cs = 64\n",
        "\n",
        "train = SLM_dataset(train_dataset, cs)\n",
        "val = SLM_dataset(val_dataset, cs)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=bs, shuffle=True, num_workers=0)\n",
        "val_dataloader = DataLoader(val, batch_size=bs, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LDrUmU19jRPl",
      "metadata": {
        "id": "LDrUmU19jRPl"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c6f3ae9",
      "metadata": {
        "id": "6c6f3ae9"
      },
      "source": [
        "We import our model, and generation function. We then initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "08c1b368",
      "metadata": {
        "id": "08c1b368"
      },
      "outputs": [],
      "source": [
        "from model.model import GPTModel, generate\n",
        "\n",
        "context_size = 64\n",
        "\n",
        "model = GPTModel(block_size=context_size, vocab_size=1000, n_embd=512, n_head=8, n_layer=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac871ca3",
      "metadata": {
        "id": "ac871ca3"
      },
      "source": [
        "We can load saved weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c438a7",
      "metadata": {
        "id": "59c438a7",
        "outputId": "2415beb0-1b90-4a4b-f719-993a1f727411"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = \"test1_epoch6.pt\"  # specify your checkpoint file here\n",
        "model.load_state_dict(torch.load(path_checkpoints + checkpoint, map_location=device, weights_only= False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eguBHnELjphl",
      "metadata": {
        "id": "eguBHnELjphl"
      },
      "source": [
        "We now perform 100 epochs of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vt3os2o3jpRz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt3os2o3jpRz",
        "outputId": "7bbbd072-f3da-43fa-b096-ba3515123744"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231/231 [00:19<00:00, 11.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 2.6492 | Val Loss = 3.0657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231/231 [00:20<00:00, 11.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss = 2.5806 | Val Loss = 3.0546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231/231 [00:19<00:00, 11.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss = 2.5485 | Val Loss = 3.0481\n",
            "Training completato!\n"
          ]
        }
      ],
      "source": [
        "from model.train import train\n",
        "\n",
        "model, train_loss, val_loss = train(model, train_dataloader, val_dataloader,\n",
        "                                    lr = 1e-4, weight_decay=1e-3, epochs=3,\n",
        "                                    opt_name=\"adam\", device=device,\n",
        "                                    checkpoint_path=path_checkpoints, name = \"new_tkn_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bed8986",
      "metadata": {
        "id": "8bed8986"
      },
      "source": [
        "Here we load the saved dicts, needed for encoding and decoding during generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0812a17e",
      "metadata": {
        "id": "0812a17e"
      },
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
        "    stoi, itos, merges, dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K01emckgztFa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "K01emckgztFa",
        "outputId": "d77d151a-d511-42d3-c62e-bf1b66cd6532"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" I'm going to buy a new pair of shoes . @ surf the second . @ Because the soft boiling is better . @ OK . @ Very good . @ Bye . @ Yeah , right ? I'm glad you know . S\""
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model, \" I'm going to buy a new pair of shoes . @ \",\n",
        "         50, stoi, itos, merges, context_size, conversation=False, temperature= 0.6, top_k=100, top_p = 0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5070a000",
      "metadata": {},
      "source": [
        "## Model Training via Lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "902631f7",
      "metadata": {},
      "source": [
        "We import our model, and generation function. We then initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39d32c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "d38d9628",
      "metadata": {},
      "outputs": [],
      "source": [
        "from model.lightning_model import GPTLightningModule\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "model = GPTLightningModule(\n",
        "    block_size=64,\n",
        "    vocab_size=1000,\n",
        "    n_embd=384,\n",
        "    n_head=6,\n",
        "    n_layer=6,\n",
        "    use_qat=False,  # Default\n",
        "    learning_rate = 1e-4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "19b8bd28",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type     | Params | Mode\n",
            "------------------------------------------\n",
            "0 | model | GPTModel | 11.4 M | eval\n",
            "------------------------------------------\n",
            "11.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.4 M    Total params\n",
            "45.765    Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "78        Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58fb12d0acc64d8d8acb87e1b746d134",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/simone/miniconda3/envs/py312/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
            "/home/simone/miniconda3/envs/py312/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
            "/home/simone/miniconda3/envs/py312/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 79 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62fa5cd4813e402b801b9d87ad5bb9f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0217cdf2c63410796f0a22454af3c70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aa4d2f0635b490896edc9277c352ebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c815bca224940438c8ab65302b7f1fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "457099058e9648e895cd534d5a182338",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "039441237190499bb0776a4ebd3c9504",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40405b3e30ad4507b4eab6735e6d0669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6138b4d12a7f424494a0a983232d98eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96349792114f47c09217309ce539f276",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79c976d4dde143e482f1c0dc6c502d82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c06b82803b9a4a52aed3a5366aeac05d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1de567c2fc0c45a2b8521884c70d926d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d5b07bf16a24766b030d8537c07c878",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76cc132a61694267bd93448e12dc3206",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06d92c261465481ca3551c138504af8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28a4c223c7d74ca08da5c31777445845",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cbe76a71a3942c5bdd640ed97c0427a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6075322a4e0842b898c5d537d454a486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e68cffa5db44ec2a9c4115cfbf08c60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566fc6f0322c48ffb5ae1f0a9af99076",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5bf60f3a731498191a42c752555d151",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(max_epochs=20, accelerator='gpu')\n",
        "trainer.fit(model, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1ddc44f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(path + 'stoi_itos_merges_dataset.pkl', \"rb\") as f:  # Python 3: open(..., 'rb')\n",
        "    stoi, itos, merges, dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "94b7332a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello , how are you ? @ clock B Remember ? ! The clock is rich ! @ What are you going to have to do ? ATom ! We have any party to water ? @ Do you have the lass the music ? months \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizer.tokenizer import decode, encode\n",
        "input_ids = torch.tensor(encode(\"Hello , how are you ? @ \", merges, stoi, len(stoi), len(merges)))\n",
        "input_ids = input_ids.reshape(1, -1)\n",
        "out = model.generate(input_ids, 50, temperature=0.7, top_k=50)\n",
        "print(decode(out[0].tolist(), itos)+ \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "py312"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
