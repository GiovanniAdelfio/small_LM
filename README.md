
   SMALL LANGUAGE MODEL TRAINED ON DAILY_DIALOG

Project Summary
---------------
This project focuses on building a small language model (LM) 
trained on a subset of the DailyDialog dataset. By limiting the 
scope and vocabulary of the dataset—similar to the approach 
used in the TinyStories project—we reduce the variety of words 
and expressions the model needs to learn.

Objectives
----------
- Train a compact LM to capture everyday conversational patterns.
- Reduce the model size using various compression techniques.
- Enable local inference on devices with limited computational power.

Methodology
------------
1. Dataset Preparation:
   Select and preprocess a smaller, focused subset of DailyDialog.

2. Model Training:
   Train or fine-tune a small language model on the prepared data.

3. Model Optimization:
   Apply model compression methods such as pruning, quantization, 
   and knowledge distillation.

4. Evaluation:
   Measure the trade-offs between model size and performance.

5. Deployment:
   Test the optimized model on low-power hardware or edge devices.

------------------------------------------------------------
In essence, this project aims to make conversational AI models 
lightweight and accessible for resource-constrained environments.
------------------------------------------------------------
